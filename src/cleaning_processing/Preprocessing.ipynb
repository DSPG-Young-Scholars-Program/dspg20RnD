{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for use with Topic Models\n",
    "\n",
    "The text has already been cleaned.  This script will preprocess it - tokenize, remove stop words, add bigrams and trigrams, lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import TextCleaning\n",
    "import LDAvariables\n",
    "import stanza\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved df.  df['working_abstract'] contains clean text.\n",
    "df = pd.read_pickle(\"./clean_dataset.pkl\")\n",
    "df.reset_index(inplace = True)\n",
    "df.rename(columns={'index':'original index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-29 13:54:21 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-06-29 13:54:21 INFO: Use device: cpu\n",
      "2020-06-29 13:54:21 INFO: Loading: tokenize\n",
      "2020-06-29 13:54:21 INFO: Loading: pos\n",
      "2020-06-29 13:54:22 INFO: Loading: lemma\n",
      "2020-06-29 13:54:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "wa='working_abstract'\n",
    "\n",
    "#Create a pipeline for lemmatizing working abstract. This pipeline will tokenize, determine pos, and then lemmatize the token appropriately.\n",
    "nlp = stanza.Pipeline(lang='en',processors='tokenize,pos,lemma',tokenize_batch_size=500,lemma_batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "#Helper Functions\n",
    "#############################################\n",
    "\n",
    "#This function was used previously, but we were unaware it had already been included in 'Cleaning', and is redundant.        \n",
    "#def remove_institution(record):\n",
    "#    \"\"\"removes all instances of exact institution name from lowercase abstract string\"\"\"\n",
    "#    org=record['ORGANIZATION_NAME']\n",
    "#    if pd.notnull(org):\n",
    "#        return record['working_abstract'].replace(org.lower(),'')\n",
    "#    else:\n",
    "#        return record['working_abstract']\n",
    "    \n",
    "def remove_custom_words(record,col_to_clean):\n",
    "    \"\"\"Designates stopwords for a particular abstract that contain embedded info e.g. author names and removes them from a lowercase abstract\"\"\"\n",
    "    fields_to_replace=[]\n",
    "    if type(record[col_to_clean])!=list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        #Main PI\n",
    "        #Adds all words in the pis names, excluding initials (hence why the commas and periods must be replaced)\n",
    "        if pd.notnull(record['CONTACT_PI_PROJECT_LEADER']):\n",
    "            fields_to_replace.extend([x.lower() for x in record['CONTACT_PI_PROJECT_LEADER'].replace(',','').replace('.','').replace('-',' ').split() if len(x)>1])\n",
    "        #Additional PIs\n",
    "        #For each pi, which are split by semicolons, and format is last,first;  #Sometimes a middle initial\n",
    "        if pd.notnull(record['OTHER_PIS']):\n",
    "            for i in record['OTHER_PIS'].split(';'):\n",
    "                i=i.strip() #Remove whitespace\n",
    "                i=i.replace('.','')#Periods for initials\n",
    "                i=i.replace(',','')#Commas between last, first\n",
    "                i=i.replace('-',' ')#Remove hyphen in hypenated names to make separate words once tokens.\n",
    "                fields_to_replace.extend([x.lower().strip() for x in i.split() if len(x)>1])\n",
    "        return [x.lower() for x in record[col_to_clean] if not x.lower() in fields_to_replace]\n",
    "\n",
    "def remove_first_x_tokens(tokened_abstract,bad_start_phrases,max_tokens_to_skip=3):\n",
    "    \"\"\"removes each bad_start_phrase occuring within max_tokens_to_skip of the front--phrases must be lowered.\n",
    "    be careful calling this, as order matters! It always starts looking at the first token, which will change between runs.\n",
    "    both tokened_abstract and each phrase in bad_start_phrases must be a list, not just a string\n",
    "    eg the phrase 'overall project summary' and 'technical abstract' should be input as a list of lists: [ ['overall','project','summary'],['technical','abstract']] \"\"\"\n",
    "    assert [type(phrase)==list for phrase in bad_start_phrases] #Make sure not just a string\n",
    "    assert [type(tokened_abstract)==list]\n",
    "    if type(tokened_abstract)!=list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        for token_sequence in bad_start_phrases:\n",
    "            #Look for a match within up to 3 tokens from the start. The reasoning here is some abstract start with numbers indicating sections\n",
    "            #EG 8., 8.a, 8.1.1.--from EDA of first tokens\n",
    "            for idx in range(0,max_tokens_to_skip):\n",
    "                if tokened_abstract[idx:len(token_sequence)+idx]==token_sequence:\n",
    "                    tokened_abstract=tokened_abstract[len(token_sequence)+idx:]\n",
    "                    break\n",
    "        return tokened_abstract\n",
    "\n",
    "#Original list used to remove--must be updated now that lemmatization occurs before\n",
    "#start_phrases_to_remove=[['section'],['abstract'],['contact','pd','pi'],['technical'],['nontechnical'],['non','technical'],\n",
    "#                         ['project','summary','abstract'], ['overall','project','summary'],['project','abstract'],\n",
    "#                        ['project','narrative'],['abstract'],['summary'],['description','provided','by','the','applicant'],\n",
    "#                         ['description','provided','by','applicant'],['description','provided','by','candidate'],\n",
    "#                         ['provided','by','investigator'], ['provided','by','the','investigator'],['description']]\n",
    "\n",
    "start_phrases_to_remove=[['section'],['abstract'],['contact','pd','pi'],['nontechnical'],['non','technical'], ['non-technical'],['technical'],\n",
    "                         ['project','summary','abstract'], ['overall','project','summary'],['project','abstract'],\n",
    "                        ['project','narrative'],['abstract'],['summary'],['description','provide','applicant'],\n",
    "                         ['description','provide','applicant'],['description','provide','candidate'],\n",
    "                         ['provide','investigator'],['description']]\n",
    "\n",
    "\"\"\"\n",
    "#Prior function which used the spacy module--but spacy is not 'research-grade'\n",
    "def lemmatize_spacy(doc,punctuation_or_token='token'):\n",
    "\"\"\"\n",
    "\"\"\"use spacy to lemmatize a document. token takes a list of strings and is then turned into a string once again. Punctuation takes one string with punctuation and parses by sentence\"\"\"\n",
    "\"\"\"\n",
    "    assert punctuation_or_token in ['token','punctuation']\n",
    "    if punctuation_or_token=='token':\n",
    "        sentence=sp(' '.join(doc))\n",
    "    elif punctuation_or_token=='punctuation':\n",
    "        sentence=sp(doc)\n",
    "    new_tokens=[]\n",
    "    for word in sentence:\n",
    "        if word.pos_ in ['NOUN','VERB','ADJ','ADV']:\n",
    "            new_tokens.append(word.lemma_)\n",
    "        elif word.pos_ in ['PROPN','NUM','X','INTJ']:\n",
    "            new_tokens.append(word.text)\n",
    "    return new_tokens\n",
    "\"\"\"    \n",
    "def lemmatize_stanford(doc,pretokened=False,keep_numbers=True):\n",
    "    \"\"\"if pretokened, dont use this function, as it hasnt been adapted for it\"\"\"\n",
    "    ##to compare lemmatization functions, try test cases 463, 40, and 2247 (iloc)\n",
    "    assert not pretokened #If these are already tokened per another pipeline, this function won't work correctly\n",
    "    new_tokens=[]\n",
    "    if doc==' ': #Quirk that somehow two empty abstracts were not caught\n",
    "        return np.nan #Produces null abstracts that can mess up your code if you're not careful\n",
    "    else: \n",
    "        processed=nlp(doc)\n",
    "        for sent in processed.sentences:\n",
    "            for word in sent.words:\n",
    "                #If its a regular noun, verb, adj, or adverb, keep lemmatized form\n",
    "                if word.pos in ['NOUN','VERB','ADJ','ADV']:\n",
    "                    new_tokens.append(word.lemma)\n",
    "                #If you decided to retain numbers, their lemma is kept here. Note that number catching isnt perfect by this lemmatizing.\n",
    "                elif word.pos=='NUM' and keep_numbers:\n",
    "                    new_tokens.append(word.lemma)\n",
    "                #Exact phrases are kept here with no attempt at lemmatization: e.g. mars does not become mars, and hopefully scientific words e.g. chemicals will be tagged as propn, x, or intj if needed\n",
    "                elif word.pos in ['PROPN','X','INTJ']: \n",
    "                    new_tokens.append(word.text)\n",
    "                #Note that no other tokens are kept\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "def create_stopwords():\n",
    "    \"\"\" creates list of stopwords. stop words include the general English list and any additional we see sneaking through\n",
    "    #We no longer remove words specific to the corpus that do not aid in meaning like science/research\"\"\"\n",
    "    \n",
    "    stopWords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # format stop words the same way we formatted our corpus, ie. without apostrophes.  \n",
    "    stop_wds = stopWords.copy()\n",
    "    for word in stopWords:\n",
    "        if \"\\'\" in word:\n",
    "            stop_wds.discard(word)\n",
    "            stop_wds.add(word.replace(\"\\'\",\"\"))\n",
    "    \n",
    "    # more stop words that do not add meaning to topics\n",
    "    additional_stopwords=['another','well','addition', 'thus',\n",
    "                      'specifically', 'similar','including',\n",
    "                       'via','within', 'thus', 'particular', 'furthermore','include','also',\n",
    "                      'includes','however','whether','due', 'may','overall', 'whether','could',\n",
    "                      'many','finally', 'several', 'specific', 'additional', 'therefore', 'either', 'various',\n",
    "                       'within', 'among', 'would'] \n",
    "        \n",
    "    sw = stop_wds.union(additional_stopwords)\n",
    "    \n",
    "    return sw\n",
    "\n",
    "def remove_stopwords(doc, stop_words):\n",
    "    \"\"\"remove stopwords\"\"\"\n",
    "    #If no acceptable tokens, this is np.nan\n",
    "    if type(doc)!=list:\n",
    "        return np.nan\n",
    "    \n",
    "    return [word for word in doc if word not in stop_words] \n",
    "\n",
    "def apply_n_grams(abstract,function):\n",
    "    \"\"\"apply an n-gram--could be bi or tri gram--to abstract\"\"\"\n",
    "    #Non lists--aka no acceptable tokens\n",
    "    if type(abstract)==list:\n",
    "        return function[abstract]\n",
    "    else:\n",
    "        return abstract\n",
    "\n",
    "\n",
    "    import re\n",
    "\n",
    "def clean_up_tokens(doc):\n",
    "    \"\"\"determines for each doc which tokens to clean up formatting further in keep_token, and decides which of these cleaned up tokens will be kept\"\"\"\n",
    "    kept_tokens=[]\n",
    "    #Ignores documents that are abstracts with no valid tokens\n",
    "    if type(doc)!=list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        for token in doc:\n",
    "            keep,altered_token=keep_token(token)\n",
    "            if keep:\n",
    "                kept_tokens.append(altered_token)\n",
    "        return kept_tokens\n",
    "        \n",
    "def keep_token(token):\n",
    "    \"\"\"strips hyphens, replaces internal hyphens with _, turns non-alphanumeric tokens into alphanumerics', strips leading _ if produced by alphamumeric,\n",
    "    then removes those updated tokens that: are numeric but not length 4, are tokens related to college names (see below) or are less than length 2.\"\"\"\n",
    "    token=token.strip('- ') #Removes leading and trailing hyphens\n",
    "    token=token.replace('-','_')\n",
    "    if not str.isalnum(token):\n",
    "        token=re.sub(r'\\W+', '', token)\n",
    "    token=token.strip('_')\n",
    "    #Names of universities\n",
    "    if 'university' in token or 'college' in token or 'universities' in token:\n",
    "        return (not token in college_tokens, token)\n",
    "    if str.isnumeric(token):\n",
    "        #keep years\n",
    "        return (len(token)==4,token)\n",
    "    else:\n",
    "        #Keep anything that is alphanumeric or alpha if its over length 2--allows mixed types e.g. h1n1\n",
    "        return (len(token)>=2,token)\n",
    "\n",
    "\n",
    "\n",
    "#Any specific university word is removed--schools within college/university, college names, etc. that cannot apply to multiple schools\n",
    "#This list was generated from all tokens that contained the string 'college','university',or 'universities'. The commented out parts of the list are terms\n",
    "#That could be considered \"generic\" ie apply to more than one school\n",
    "college_tokens=[#'college',\n",
    "'aga_khan_university',\n",
    "'ahmadu_bello_university',\n",
    "'alabama_aamp_university',\n",
    "'albert_einstein_college',\n",
    "'alcorn_state_university',\n",
    "'american_college_obstetricians',\n",
    "'american_college_surgeons',\n",
    "'americancollege',\n",
    "#'amongcollege',\n",
    "#'anduniversity',\n",
    "'anne_molloy_trinity_college',\n",
    "#'atuniversity',\n",
    "'auburn_university',\n",
    "'auburn_university_alabama',\n",
    "'auburn_university_au',\n",
    "'auburn_university_auburn',\n",
    "'auburn_university_montgomery',\n",
    "'auburn_university_tuskegee_university',\n",
    "'augustana_college',\n",
    "#'auniversity',\n",
    "'babes_bolyai_university',\n",
    "'barnard_college',\n",
    "'baruch_college',\n",
    "'bates_college',\n",
    "'baylor_college',\n",
    "'baylor_college_dentistry',\n",
    "'baylor_college_medicine',\n",
    "'baylor_college_ofmedicine',\n",
    "'baylor_collegeof',\n",
    "'baylorcollege_medicine',\n",
    "'baylorcollege_medicine_bcm',\n",
    "'ben_gurion_university',\n",
    "'benedict_college',\n",
    "'benedict_college_historically_black',\n",
    "'berea_college',\n",
    "'binghamton_university',\n",
    "#'black_colleges',\n",
    "'board_trinity_college',\n",
    "'bostonuniversity',\n",
    "'bowdoin_college',\n",
    "'brownuniversity',\n",
    "'bryn_mawr_college',\n",
    "'bucknell_university',\n",
    "#'cape_universities',\n",
    "'cardiff_university',\n",
    "'carleton_college',\n",
    "'carnegie_mellon_university',\n",
    "'carver_college',\n",
    "'carver_college_medicine',\n",
    "'case_western_reserveuniversity',\n",
    "'case_westernreserve_university',\n",
    "'catholic_university',\n",
    "'cerritos_college',\n",
    "'charles_drew_university',\n",
    "'chulalongkorn_university',\n",
    "'chulalongkorn_university_bangkok_thailand',\n",
    "'claflin_university',\n",
    "'claremont_colleges',\n",
    "'clark_atlanta_university',\n",
    "'colby_college',\n",
    "'colby_sawyer_college',\n",
    "#'college',\n",
    "#'college',\n",
    "#'college_american_pathologists',\n",
    "#'college_arts',\n",
    "#'college_arts_sciences',\n",
    "'college_brockport',\n",
    "'college_dentistry_nyucd',\n",
    "'college_dentistry_ufcd',\n",
    "#'college_goer',\n",
    "#'college_graduates',\n",
    "#'college_letters',\n",
    "#'college_letters_arts_sciences',\n",
    "#'college_letters_sciences',\n",
    "'college_lewiston',\n",
    "#'college_liberal_arts',\n",
    "'college_london',\n",
    "'college_medicine_aecom',\n",
    "'college_medicine_uccom',\n",
    "'college_menominee_nation',\n",
    "#'college_optometry',\n",
    "#'college_osteopathic_medicine',\n",
    "'college_park_umcp',\n",
    "'college_park_umd',\n",
    "#'college_physicians_surgeons',\n",
    "#'college_rheumatology_acr',\n",
    "'college_south_hadley',\n",
    "#'college_sports_medicine',\n",
    "'college_st_scholastica',\n",
    "'college_staten_island',\n",
    "#'college_students_basics',\n",
    "#'college_veterinary_medicine',\n",
    "#'college_veterinary_pathologists',\n",
    "#'college_veterinarymedicine',\n",
    "'college_wcmc',\n",
    "'college_wcmc_rockefeller_university',\n",
    "'college_william_mary',\n",
    "'college_wisconsin_mcw',\n",
    "'college_wooster',\n",
    "#'collegeand',\n",
    "#'collegeof',\n",
    "#'collegeof_medicine',\n",
    "#'colleges',\n",
    "#'colleges_arts_sciences',\n",
    "#'colleges_chicago',\n",
    "#'colleges_dentistry',\n",
    "#'colleges_dentistry_medicine',\n",
    "#'colleges_optometry',\n",
    "'colleges_rcc_umb', #Iffy--not sure what this is\n",
    "#'colleges_schools',\n",
    "#'colleges_universities',\n",
    "#'colleges_universities_hacu',\n",
    "#'collegesand',\n",
    "#'collegestudent',\n",
    "'columbia_university',\n",
    "'columbiauniversity',\n",
    "'comanche_nation_college',\n",
    "#'communitycollege',\n",
    "'creighton_university',\n",
    "'cross_university',\n",
    "'cross__university',\n",
    "'cuny_hunter_college',\n",
    "'del_mar_college',\n",
    "'depaul_university',\n",
    "'dine_college',\n",
    "'din_college',\n",
    "'diné_college',\n",
    "'diplomate_american_college',\n",
    "'doane_college',\n",
    "'doron_levy_university_maryland',\n",
    "'dukeuniversity',\n",
    "'eckerd_college',\n",
    "'emoryuniversity',\n",
    "'famu_fsu_college',\n",
    "'fort_lewis_college',\n",
    "'franklin_marshall_college',\n",
    "'fudan_university',\n",
    "'fudan_university_shanghai',\n",
    "'fudan_university_shanghai_china',\n",
    "'gallaudet_university',\n",
    "'george_mason_university',\n",
    "'george_washington_university',\n",
    "'georgetown_howard_universities',\n",
    "'georgetown_university',\n",
    "'georgia_regents_university',\n",
    "'gettysburg_college',\n",
    "#'grant_universities_aplu',\n",
    "'gu_howard_university',\n",
    "'hackensack_university',\n",
    "'hampton_university',\n",
    "'hanyang_university',\n",
    "'hartnell_college',\n",
    "'harvarduniversity',\n",
    "'harvey_mudd_college',\n",
    "#'historically_black_college',\n",
    "#'historically_black_colleges',\n",
    "#'historically_black_colleges_universities',\n",
    "'hokkaido_university',\n",
    "'hold_bates_college',\n",
    "'hold_colby_sawyer_college',\n",
    "'hold_stonehill_college_easton',\n",
    "'honors_college',\n",
    "'houston_baylor_college',\n",
    "'hunter_college',\n",
    "'imperial_college',\n",
    "'imperial_college_london',\n",
    "'imperial_college_london_uk',\n",
    "#'incollege',\n",
    "'indiana_university',\n",
    "'indianauniversity',\n",
    "#'inspect_certified_college',\n",
    "#'inter_college',\n",
    "#'inter_university',\n",
    "#'inter_university_consortium_political',\n",
    "#'interuniversity',\n",
    "#'interuniversity_consortium_political',\n",
    "#'intra_university',\n",
    "'james_cook_university',\n",
    "'james_madison_university',\n",
    "'jeffersonuniversity',\n",
    "'john_jay_college',\n",
    "'johns_hopkinsuniversity',\n",
    "'kennesaw_state_university',\n",
    "'king_college_london',\n",
    "'kwame_nkrumah_university',\n",
    "'kyoto_university',\n",
    "'kyushu_university',\n",
    "'langston_university',\n",
    "'lehman_college',\n",
    "'lehman_college_city',\n",
    "'lehman_college_cuny',\n",
    "'lemoyne_owen_college',\n",
    "'lewis_clark_college',\n",
    "#'liberal_art_college',\n",
    "'louisiana_universities_marine',\n",
    "'loyola_marymount_university',\n",
    "'loyola_university',\n",
    "'loyola_university_chicago',\n",
    "'macalester_college',\n",
    "'makerere_university',\n",
    "'makerere_university_kampala_uganda',\n",
    "'makerere_university_uganda',\n",
    "'makerereuniversity',\n",
    "'marquette_university',\n",
    "'marquette_university_milwaukee',\n",
    "'mbarara_university',\n",
    "'mcgill_university',\n",
    "'mcmaster_university',\n",
    "'medgar_evers_college',\n",
    "'medical_colleges_aamc',\n",
    "'medicalcollege',\n",
    "'medicaluniversity_south_carolina',\n",
    "'medicine_yeshiva_university',\n",
    "'meharrymedical_college',\n",
    "'mellon_university',\n",
    "'mellonuniversity',\n",
    "'mexico_highlands_university',\n",
    "'miami_dade_college',\n",
    "'middlebury_college',\n",
    "'millsaps_college',\n",
    "'monash_university',\n",
    "'monash_university_australia',\n",
    "#'montana_tribal_college',\n",
    "#'montana_tribal_colleges',\n",
    "'montclair_state_university',\n",
    "'morehouse_college',\n",
    "'morehouse_college_spelman_college',\n",
    "'mount_holyoke_college',\n",
    "'msm_tuskegee_university',\n",
    "'mt_marty_college',\n",
    "'muhimbili_university',\n",
    "#'multi_university',\n",
    "#'muniversity',\n",
    "'nakoda_college',\n",
    "'nanyang_technological_university',\n",
    "'nazarene_university',\n",
    "'nazareth_college',\n",
    "#'non_college',\n",
    "#'non_university',\n",
    "'northern_arizona_university',\n",
    "'northern_kentucky_university',\n",
    "'northshore_university',\n",
    "'northshore_university_healthsystem',\n",
    "'northwest_nazarene_university',\n",
    "'northwestern_university',\n",
    "'norwich_university',\n",
    "#'ofuniversity',\n",
    "'oglala_lakota_college',\n",
    "'ohio_stateuniversity',\n",
    "'old_dominion_university',\n",
    "'olin_college',\n",
    "#'otheruniversity',\n",
    "#'participatinguniversity',\n",
    "'pasadena_city_college',\n",
    "'peking_university',\n",
    "'peking_university_beijing_china',\n",
    "'pennsylvania_college_optometry',\n",
    "#'phduniversity',\n",
    "#'polytechnic_university',\n",
    "#'post__college',\n",
    "'prairie_view_university',\n",
    "#'pre_college',\n",
    "#'pre_university',\n",
    "#'pre__college',\n",
    "#'precollege',\n",
    "'queens_college',\n",
    "'regents_university',\n",
    "'researchuniversity',\n",
    "'rockefeller_university',\n",
    "'rockefeller_university_memorial_sloan',\n",
    "'rockefeller_university_ru',\n",
    "'rockefeller_university_weill_cornell',\n",
    "'rockefelleruniversity',\n",
    "'royal_college_surgeons',\n",
    "'rutgers_university',\n",
    "'rutgersuniversity',\n",
    "'saddleback_college',\n",
    "'saginaw_chippewa_tribal_college',\n",
    "'saint_michael_college',\n",
    "'salish_kootenai_college',\n",
    "'salve_regina_university',\n",
    "'sawyer_college',\n",
    "#'scienceuniversity', #This is likely ohsu, as bellow, but for parsimony, this is kept\n",
    "'scienceuniversity_ohsu',\n",
    "'serc_carleton_college',\n",
    "'shams_university',\n",
    "'shams_university_cairo_egypt',\n",
    "'shanghai_jiaotong_university',\n",
    "'simon_fraser_university',\n",
    "'sinte_gleska_university',\n",
    "'sisseton_wahpeton_college',\n",
    "'sitting_bull_college',\n",
    "'skc_tribal_college',\n",
    "'sokoine_university',\n",
    "'south_africa_university_witwatersrand',\n",
    "'southern_illinois_university_carbondale',\n",
    "'southern_illinois_university_edwardsville',\n",
    "'southern_methodist_university',\n",
    "'spelman_college',\n",
    "'st_edward_university',\n",
    "'st_mary_college',\n",
    "'st_olaf_college',\n",
    "'st_philip_college',\n",
    "'stanforduniversity',\n",
    "'state_university_dominguez', #Specific university\n",
    "#'stateuniversity', #This could be any state\n",
    "'stellenbosch_university',\n",
    "'stellenbosch_university_south_africa',\n",
    "'stonehill_college',\n",
    "'stonehill_college_easton_massachusetts',\n",
    "'stony_brook_university',\n",
    "'swarthmore_college',\n",
    "'tarrant_county_college',\n",
    "'tel_aviv_university',\n",
    "'templeuniversity',\n",
    "'texas_a_university',\n",
    "'texas_southmost_college',\n",
    "'texas_university_kingsville',\n",
    "#'thecollege',\n",
    "#'theuniversity',\n",
    "'theuniversity_california_san',\n",
    "'theuniversity_colorado',\n",
    "'theuniversity_maryland',\n",
    "'theuniversity_michigan',\n",
    "'theuniversity_minnesota',\n",
    "'theuniversity_north_carolina',\n",
    "'theuniversity_pennsylvania',\n",
    "'theuniversity_pittsburgh',\n",
    "'tougaloo_college',\n",
    "#'touniversity',\n",
    "#'triangle_universities_nuclear', #this is a government research center\n",
    "#'tribal_college',\n",
    "'tribal_college_haskell_indian', #specific university\n",
    "#'tribal_colleges',\n",
    "#'tribal_colleges_universities',\n",
    "#'tribal_colleges_universities_tcus',\n",
    "'trinity_college',\n",
    "'trinity_college_arts_sciences',\n",
    "'trinity_college_dublin',\n",
    "'tsinghua_university',\n",
    "'tsinghua_university_beijing',\n",
    "'tsinghua_university_beijing_china',\n",
    "'tsinghua_university_china',\n",
    "#'tsinghua_university_prof_roberto',\n",
    "'tulaneuniversity',\n",
    "'tuskegee_universities',\n",
    "'tuskegee_university',\n",
    "'tuskegee_university_hbcu',\n",
    "'uams_colleges',\n",
    "'ucsf_makerere_university',\n",
    "'umbc_university_maryland',\n",
    "'uniformed_services_university',\n",
    "'united_negro_college',\n",
    "#'universities',\n",
    "#'universities_aau', #this is an association of universities, not a university\n",
    "#'universities_hbcu',\n",
    "'universities_kansas_ku',\n",
    "#'universitiesand',\n",
    "#'universitiesin',\n",
    "#'university',\n",
    "#'university',\n",
    "'university_alabama_birmingham',\n",
    "'university_alabama_huntsville',\n",
    "'university_alabama_tuscaloosa',\n",
    "'university_alabama_ua',\n",
    "'university_alaska_anchorage',\n",
    "'university_alaska_fairbanks',\n",
    "'university_albany_suny',\n",
    "'university_arizona_ua',\n",
    "'university_arkansas_fayetteville',\n",
    "'university_arkansas_pine',\n",
    "'university_arkansas_ua',\n",
    "'university_buffalo_suny',\n",
    "'university_buffalo_ub',\n",
    "'university_california_berkeley',\n",
    "'university_california_davis',\n",
    "'university_california_irvine',\n",
    "'university_california_los',\n",
    "'university_california_merced',\n",
    "'university_california_riverside',\n",
    "'university_california_san',\n",
    "'university_california_sanfrancisco',\n",
    "'university_california_santa',\n",
    "'university_cincinnati_cincinnati',\n",
    "'university_college_dublin',\n",
    "'university_college_london',\n",
    "'university_colorado_anschutz',\n",
    "'university_colorado_boulder',\n",
    "'university_colorado_denver',\n",
    "'university_connecticut_uconn',\n",
    "'university_feinberg_school',\n",
    "'university_florida_gainesville',\n",
    "'university_florida_uf',\n",
    "'university_fullerton_csuf',\n",
    "'university_georgia_athens',\n",
    "'university_georgia_uga',\n",
    "'university_hawaii_hilo',\n",
    "'university_hawaii_manoa',\n",
    "'university_hawaii_uh',\n",
    "'university_hospitals_cleveland',\n",
    "'university_houston_downtown',\n",
    "'university_houston_uh',\n",
    "'university_illinois_chicago',\n",
    "'university_illinois_urbana',\n",
    "'university_indianapolis_iupui',\n",
    "'university_kansas_ku',\n",
    "'university_kansas_lawrence',\n",
    "'university_kingsville',\n",
    "'university_langone_medical',\n",
    "'university_louisiana_lafayette',\n",
    "'university_louisiana_monroe',\n",
    "'university_maryland',\n",
    "'university_maryland_baltimore',\n",
    "'university_maryland_baltimore_county',\n",
    "'university_maryland_baltimore_umb',\n",
    "'university_maryland_eastern_shore',\n",
    "'university_maryland_greenebaum',\n",
    "'university_maryland_marlene_stewart', \n",
    "'university_maryland_umd',\n",
    "'university_massachusetts_amherst',\n",
    "'university_massachusetts_dartmouth',\n",
    "'university_massachusetts_lowell',\n",
    "'university_massachusetts_umass',\n",
    "'university_miami_miller',\n",
    "'university_miami_um',\n",
    "'university_michigan_ann',\n",
    "'university_michigan_dearborn',\n",
    "'university_michigan_um',\n",
    "'university_minnesota',\n",
    "'university_minnesota_duluth',\n",
    "'university_minnesota_masonic',\n",
    "'university_minnesota_minneapolis',\n",
    "'university_minnesota_twin',\n",
    "'university_minnesota_umn',\n",
    "'university_missouri__columbia',\n",
    "'university_missouri_columbia',\n",
    "'university_missouri_kansas',\n",
    "'university_missouri_mu',\n",
    "'university_missouri_rolla',\n",
    "'university_missouri_st',\n",
    "'university_nebraska_lincoln',\n",
    "'university_nebraska_omaha',\n",
    "'university_nevada_las',\n",
    "'university_nevada_reno',\n",
    "'university_northcarolina_chapel',\n",
    "'university_northridge_csun',\n",
    "'university_ofalabama',\n",
    "'university_ofcalifornia',\n",
    "'university_ofcolorado',\n",
    "'university_ofmichigan',\n",
    "'university_ofminnesota',\n",
    "'university_ofpennsylvania',\n",
    "'university_ofrochester',\n",
    "'university_oftexas',\n",
    "'university_ofwashington',\n",
    "'university_ofwashington_uw',\n",
    "'university_ofwisconsin',\n",
    "'university_ofwisconsin_madison',\n",
    "'university_oklahoma_norman',\n",
    "'university_oklahoma_ou',\n",
    "'university_pennsylvania_upenn',\n",
    "'university_pittsburgh_pitt',\n",
    "'university_singapore_nus',\n",
    "'university_singapore_singapore',\n",
    "'university_tennessee_chattanooga',\n",
    "'university_tennessee_knoxville',\n",
    "'university_tennessee_memphis',\n",
    "'university_texas_arlington',\n",
    "'university_texas_austin',\n",
    "'university_texas_brownsville',\n",
    "'university_texas_dallas',\n",
    "'university_texas_el',\n",
    "'university_texas_pan',\n",
    "'university_texas_rio',\n",
    "'university_texas_southwestern',\n",
    "'university_texas_tyler',\n",
    "'university_toronto_toronto',\n",
    "'university_venda',\n",
    "'university_vermont_burlington',\n",
    "'university_vermont_uvm',\n",
    "'university_virginia_charlottesville',\n",
    "'university_virginia_uva',\n",
    "'university_washington_seattle',\n",
    "'university_washington_uw',\n",
    "'university_waterloo',\n",
    "'university_west_indies',\n",
    "'university_wisconsin_carbone',\n",
    "'university_wisconsin_eau',\n",
    "'university_wisconsin_madison',\n",
    "'university_wisconsin_milwaukee',\n",
    "'university_wisconsin_oshkosh',\n",
    "'university_wisconsin_platteville',\n",
    "'university_wisconsin_stout',\n",
    "'university_witwatersrand',\n",
    "'university_witwatersrand_south_africa',\n",
    "'university_witwatersrand_wits',\n",
    "#'universityabstract',\n",
    "#'universityand',\n",
    "#'universitycareer',\n",
    "#'universityco',\n",
    "#'universityhospitals',\n",
    "#'universityin',\n",
    "#'universityintellectual',\n",
    "#'universitymedical',\n",
    "#'universityof',\n",
    "'universityof_california_san',\n",
    "'universityof_chicago',\n",
    "'universityof_colorado',\n",
    "'universityof_kentucky',\n",
    "'universityof_michigan',\n",
    "'universityof_minnesota',\n",
    "'universityof_pennsylvania',\n",
    "'universityof_pittsburgh',\n",
    "'universityof_washington',\n",
    "#'universityproposal',\n",
    "#'universityresources',\n",
    "#'universitys',\n",
    "#'universityschool_medicine',\n",
    "#'universitytitle',\n",
    "'urmc_college_arts',\n",
    "'vanderbiltuniversity',\n",
    "'virginia_commonwealth_university',\n",
    "'wake_forest_university',\n",
    "'washingtonuniversity',\n",
    "'wayne_stateuniversity',\n",
    "'weinberg_college_arts',\n",
    "'wellesley_college',\n",
    "'wesley_college',\n",
    "'western_ontario_mcmaster_universities',\n",
    "'western_ontario_mcmasters_universities',\n",
    "'westminster_college',\n",
    "#'withuniversity',\n",
    "'xiamen_university',\n",
    "'xiamen_university_china',\n",
    "'yaleuniversity',\n",
    "'yeshiva_university',\n",
    "'yonsei_university',\n",
    "'yonsei_university_seoul_south',\n",
    "'yorkuniversity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: There is an important change in order from past runs of the code here. Now, lemmatization and tokenization occurs within the same step\n",
    "#Removal of pi names, 'description provided by the applicant' and the creation of n-grams occurs AFTER lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original index</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>RAW_ABSTRACT</th>\n",
       "      <th>FY</th>\n",
       "      <th>FIRST_CHAR</th>\n",
       "      <th>LAST_CHAR</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>IC_CENTER</th>\n",
       "      <th>PROJECT_NUMBER</th>\n",
       "      <th>...</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>CONTACT_PI_PROJECT_LEADER</th>\n",
       "      <th>OTHER_PIS</th>\n",
       "      <th>ORGANIZATION_NAME</th>\n",
       "      <th>CFDA_CODE</th>\n",
       "      <th>FY_TOTAL_COST</th>\n",
       "      <th>working_abstract</th>\n",
       "      <th>nchar</th>\n",
       "      <th>Start Char</th>\n",
       "      <th>Field Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89996</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>2008</td>\n",
       "      <td>This is a project to explore Game-based, Metap...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0814512</td>\n",
       "      <td>...</td>\n",
       "      <td>Achievement; analog; base; Cognitive Science; ...</td>\n",
       "      <td>REESE, DEBBIE D</td>\n",
       "      <td>CARTER, BEVERLY; WOOD, CHARLES; HITT, BEN</td>\n",
       "      <td>WHEELING JESUIT UNIVERSITY</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1999467.0</td>\n",
       "      <td>this is a project to explore game-based, metap...</td>\n",
       "      <td>2057</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>89997</td>\n",
       "      <td>Institution: Franklin Institute Science Museum...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Institution: Franklin Institute Science Museum...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0741659</td>\n",
       "      <td>...</td>\n",
       "      <td>Active Learning; Child; Computer software; des...</td>\n",
       "      <td>SNYDER, STEVEN</td>\n",
       "      <td>ELINICH, KAREN; YOON, SUSAN</td>\n",
       "      <td>FRANKLIN INSTITUTE</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1799699.0</td>\n",
       "      <td>institution:  science museum pi: snyder, steve...</td>\n",
       "      <td>1913</td>\n",
       "      <td>i</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>89998</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Through programs (including small group conver...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0813522</td>\n",
       "      <td>...</td>\n",
       "      <td>Address; Age; Birth; Brain; Caregivers; Child;...</td>\n",
       "      <td>FINK, LAURIE KLEINBAUM</td>\n",
       "      <td>CADIGAN, KAREN; ELLENBOGEN, KIRSTEN</td>\n",
       "      <td>SCIENCE MUSEUM OF MINNESOTA</td>\n",
       "      <td>47.076</td>\n",
       "      <td>1505858.0</td>\n",
       "      <td>through programs (including small group conver...</td>\n",
       "      <td>1154</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>89999</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>2008</td>\n",
       "      <td>In partnership with the American Chemical Soci...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0838627</td>\n",
       "      <td>...</td>\n",
       "      <td>Advanced Development; American; Chemicals; Che...</td>\n",
       "      <td>JOST, JOHN W</td>\n",
       "      <td>MILLER, BRADLEY; BOWMAN, KATHERINE</td>\n",
       "      <td>INTERNATIONAL UNION OF PURE AND APPLIED CHEMISTRY</td>\n",
       "      <td>47.049</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>in partnership with the american chemical soci...</td>\n",
       "      <td>826</td>\n",
       "      <td>i</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>90000</td>\n",
       "      <td>Amphibian populations around the world are exp...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Amphibian populations around the world are exp...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0815315</td>\n",
       "      <td>...</td>\n",
       "      <td>Amphibia; Central America; Communicable Diseas...</td>\n",
       "      <td>ZAMUDIO, KELLY R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CORNELL UNIVERSITY ITHACA</td>\n",
       "      <td>47.074</td>\n",
       "      <td>370996.0</td>\n",
       "      <td>amphibian populations around the world are exp...</td>\n",
       "      <td>1322</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>99</td>\n",
       "      <td>90104</td>\n",
       "      <td>Designing a response to global climate change ...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Designing a response to global climate change ...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0825915</td>\n",
       "      <td>...</td>\n",
       "      <td>Climate; climate change; cost; Costs and Benef...</td>\n",
       "      <td>WEBSTER, MORT D</td>\n",
       "      <td>JACOBY, HENRY</td>\n",
       "      <td>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</td>\n",
       "      <td>47.075</td>\n",
       "      <td>442082.0</td>\n",
       "      <td>designing a response to global climate change ...</td>\n",
       "      <td>1884</td>\n",
       "      <td>d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>90105</td>\n",
       "      <td>CBET-0838607DozierThe WATERS Network is an ini...</td>\n",
       "      <td>2008</td>\n",
       "      <td>CBET-0838607DozierThe WATERS Network is an ini...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0838607</td>\n",
       "      <td>...</td>\n",
       "      <td>Agriculture; Automobile Driving; base; Behavio...</td>\n",
       "      <td>DOZIER, JEFF</td>\n",
       "      <td>BRADEN, JOHN; SCHNOOR, JERALD; MINSKER, BARBAR...</td>\n",
       "      <td>UNIVERSITY OF CALIFORNIA SANTA BARBARA</td>\n",
       "      <td>47.041</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>cbet-0838607dozierthe waters network is an ini...</td>\n",
       "      <td>3332</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>101</td>\n",
       "      <td>90106</td>\n",
       "      <td>Lead Partner:  Michigan Technological Universi...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Lead Partner:  Michigan Technological Universi...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0831948</td>\n",
       "      <td>...</td>\n",
       "      <td>Administrator; American; Area; base; Collabora...</td>\n",
       "      <td>ROSE, WILLIAM I</td>\n",
       "      <td>BALTENSPERGER, BRADLEY; HOWARTH, JOHN; HUNTOON...</td>\n",
       "      <td>MICHIGAN TECHNOLOGICAL UNIVERSITY</td>\n",
       "      <td>47.076</td>\n",
       "      <td>3311435.0</td>\n",
       "      <td>lead partner:  core partner: grand rapids publ...</td>\n",
       "      <td>3363</td>\n",
       "      <td>l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>102</td>\n",
       "      <td>90107</td>\n",
       "      <td>The Minority Student Pipeline Math Science Par...</td>\n",
       "      <td>2008</td>\n",
       "      <td>The Minority Student Pipeline Math Science Par...</td>\n",
       "      <td>?</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0831970</td>\n",
       "      <td>...</td>\n",
       "      <td>Affect; base; Biotechnology; career; Character...</td>\n",
       "      <td>CAMPBELL, ANISHA</td>\n",
       "      <td>KATZ, BRUCE; BARROW, CHRISTINE; ELBY, ANDREW; ...</td>\n",
       "      <td>UNIVERSITY OF MARYLAND SYSTEM</td>\n",
       "      <td>47.076</td>\n",
       "      <td>7274394.0</td>\n",
       "      <td>the minority student pipeline math science par...</td>\n",
       "      <td>2964</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>104</td>\n",
       "      <td>89959</td>\n",
       "      <td>The earliest detailed records of Australia's i...</td>\n",
       "      <td>2008</td>\n",
       "      <td>The earliest detailed records of Australia's i...</td>\n",
       "      <td>.</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0844550</td>\n",
       "      <td>...</td>\n",
       "      <td>Aborigine; Agriculture; Americas; Archaeology;...</td>\n",
       "      <td>BOWERN, CLAIRE L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YALE UNIVERSITY</td>\n",
       "      <td>47.075</td>\n",
       "      <td>407272.0</td>\n",
       "      <td>the earliest detailed records of australia's i...</td>\n",
       "      <td>1699</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    original index  PROJECT_ID  \\\n",
       "0                0       89996   \n",
       "1                1       89997   \n",
       "2                2       89998   \n",
       "3                3       89999   \n",
       "4                4       90000   \n",
       "..             ...         ...   \n",
       "95              99       90104   \n",
       "96             100       90105   \n",
       "97             101       90106   \n",
       "98             102       90107   \n",
       "99             104       89959   \n",
       "\n",
       "                                         RAW_ABSTRACT    FY  \\\n",
       "0   This is a project to explore Game-based, Metap...  2008   \n",
       "1   Institution: Franklin Institute Science Museum...  2008   \n",
       "2   Through programs (including small group conver...  2008   \n",
       "3   In partnership with the American Chemical Soci...  2008   \n",
       "4   Amphibian populations around the world are exp...  2008   \n",
       "..                                                ...   ...   \n",
       "95  Designing a response to global climate change ...  2008   \n",
       "96  CBET-0838607DozierThe WATERS Network is an ini...  2008   \n",
       "97  Lead Partner:  Michigan Technological Universi...  2008   \n",
       "98  The Minority Student Pipeline Math Science Par...  2008   \n",
       "99  The earliest detailed records of Australia's i...  2008   \n",
       "\n",
       "                                           FIRST_CHAR LAST_CHAR DEPARTMENT  \\\n",
       "0   This is a project to explore Game-based, Metap...         .        NSF   \n",
       "1   Institution: Franklin Institute Science Museum...         .        NSF   \n",
       "2   Through programs (including small group conver...         .        NSF   \n",
       "3   In partnership with the American Chemical Soci...         .        NSF   \n",
       "4   Amphibian populations around the world are exp...         .        NSF   \n",
       "..                                                ...       ...        ...   \n",
       "95  Designing a response to global climate change ...         .        NSF   \n",
       "96  CBET-0838607DozierThe WATERS Network is an ini...         .        NSF   \n",
       "97  Lead Partner:  Michigan Technological Universi...         .        NSF   \n",
       "98  The Minority Student Pipeline Math Science Par...         ?        NSF   \n",
       "99  The earliest detailed records of Australia's i...         .        NSF   \n",
       "\n",
       "   AGENCY IC_CENTER PROJECT_NUMBER  ...  \\\n",
       "0     NSF       NaN        0814512  ...   \n",
       "1     NSF       NaN        0741659  ...   \n",
       "2     NSF       NaN        0813522  ...   \n",
       "3     NSF       NaN        0838627  ...   \n",
       "4     NSF       NaN        0815315  ...   \n",
       "..    ...       ...            ...  ...   \n",
       "95    NSF       NaN        0825915  ...   \n",
       "96    NSF       NaN        0838607  ...   \n",
       "97    NSF       NaN        0831948  ...   \n",
       "98    NSF       NaN        0831970  ...   \n",
       "99    NSF       NaN        0844550  ...   \n",
       "\n",
       "                                        PROJECT_TERMS  \\\n",
       "0   Achievement; analog; base; Cognitive Science; ...   \n",
       "1   Active Learning; Child; Computer software; des...   \n",
       "2   Address; Age; Birth; Brain; Caregivers; Child;...   \n",
       "3   Advanced Development; American; Chemicals; Che...   \n",
       "4   Amphibia; Central America; Communicable Diseas...   \n",
       "..                                                ...   \n",
       "95  Climate; climate change; cost; Costs and Benef...   \n",
       "96  Agriculture; Automobile Driving; base; Behavio...   \n",
       "97  Administrator; American; Area; base; Collabora...   \n",
       "98  Affect; base; Biotechnology; career; Character...   \n",
       "99  Aborigine; Agriculture; Americas; Archaeology;...   \n",
       "\n",
       "   CONTACT_PI_PROJECT_LEADER  \\\n",
       "0            REESE, DEBBIE D   \n",
       "1             SNYDER, STEVEN   \n",
       "2     FINK, LAURIE KLEINBAUM   \n",
       "3               JOST, JOHN W   \n",
       "4           ZAMUDIO, KELLY R   \n",
       "..                       ...   \n",
       "95           WEBSTER, MORT D   \n",
       "96              DOZIER, JEFF   \n",
       "97           ROSE, WILLIAM I   \n",
       "98          CAMPBELL, ANISHA   \n",
       "99          BOWERN, CLAIRE L   \n",
       "\n",
       "                                            OTHER_PIS  \\\n",
       "0           CARTER, BEVERLY; WOOD, CHARLES; HITT, BEN   \n",
       "1                         ELINICH, KAREN; YOON, SUSAN   \n",
       "2                 CADIGAN, KAREN; ELLENBOGEN, KIRSTEN   \n",
       "3                  MILLER, BRADLEY; BOWMAN, KATHERINE   \n",
       "4                                                 NaN   \n",
       "..                                                ...   \n",
       "95                                      JACOBY, HENRY   \n",
       "96  BRADEN, JOHN; SCHNOOR, JERALD; MINSKER, BARBAR...   \n",
       "97  BALTENSPERGER, BRADLEY; HOWARTH, JOHN; HUNTOON...   \n",
       "98  KATZ, BRUCE; BARROW, CHRISTINE; ELBY, ANDREW; ...   \n",
       "99                                                NaN   \n",
       "\n",
       "                                    ORGANIZATION_NAME CFDA_CODE FY_TOTAL_COST  \\\n",
       "0                          WHEELING JESUIT UNIVERSITY    47.076     1999467.0   \n",
       "1                                  FRANKLIN INSTITUTE    47.076     1799699.0   \n",
       "2                         SCIENCE MUSEUM OF MINNESOTA    47.076     1505858.0   \n",
       "3   INTERNATIONAL UNION OF PURE AND APPLIED CHEMISTRY    47.049       51000.0   \n",
       "4                           CORNELL UNIVERSITY ITHACA    47.074      370996.0   \n",
       "..                                                ...       ...           ...   \n",
       "95              MASSACHUSETTS INSTITUTE OF TECHNOLOGY    47.075      442082.0   \n",
       "96             UNIVERSITY OF CALIFORNIA SANTA BARBARA    47.041      900000.0   \n",
       "97                  MICHIGAN TECHNOLOGICAL UNIVERSITY    47.076     3311435.0   \n",
       "98                      UNIVERSITY OF MARYLAND SYSTEM    47.076     7274394.0   \n",
       "99                                    YALE UNIVERSITY    47.075      407272.0   \n",
       "\n",
       "                                     working_abstract nchar  Start Char  \\\n",
       "0   this is a project to explore game-based, metap...  2057           t   \n",
       "1   institution:  science museum pi: snyder, steve...  1913           i   \n",
       "2   through programs (including small group conver...  1154           t   \n",
       "3   in partnership with the american chemical soci...   826           i   \n",
       "4   amphibian populations around the world are exp...  1322           a   \n",
       "..                                                ...   ...         ...   \n",
       "95  designing a response to global climate change ...  1884           d   \n",
       "96  cbet-0838607dozierthe waters network is an ini...  3332           c   \n",
       "97  lead partner:  core partner: grand rapids publ...  3363           l   \n",
       "98  the minority student pipeline math science par...  2964           t   \n",
       "99  the earliest detailed records of australia's i...  1699           t   \n",
       "\n",
       "   Field Count  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            1  \n",
       "..         ...  \n",
       "95           0  \n",
       "96           0  \n",
       "97           0  \n",
       "98           0  \n",
       "99           0  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create subset \n",
    "smalldf  = df[:100]\n",
    "smalldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lemma_docs/lemma_docs_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a9fce05825d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Bring all the batches together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokened_lemma_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./lemma_docs/lemma_docs_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma_docs_with_stop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokened_lemma_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a9fce05825d6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Bring all the batches together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokened_lemma_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./lemma_docs/lemma_docs_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma_docs_with_stop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokened_lemma_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/software/standard/core/anaconda/2019.10-py3.7/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m    144\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# 1) try standard libary Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/software/standard/core/anaconda/2019.10-py3.7/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lemma_docs/lemma_docs_0.pkl'"
     ]
    }
   ],
   "source": [
    "####################\n",
    "#Lemmatize\n",
    "####################\n",
    "#In an ideal world, this would be run once. However, it takes ages, so instead, we ran it in parallel, and batches them together\n",
    "#tokened_lemma_docs=df[wa].apply(lambda x: lemmatize_stanford(x,keep_numbers=False))\n",
    "\n",
    "#Bring all the batches together\n",
    "tokened_lemma_docs=pd.concat([pd.read_pickle(\"./lemma_docs/lemma_docs_\"+str(idx)+\".pkl\") for idx in range(20)],ignore_index=True)\n",
    "df['lemma_docs_with_stop']=tokened_lemma_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Additional cleaning post lemmatizing:\n",
    "#abstracts with no tokens become np.nan\n",
    "#remove_custom_words: Abstracts have the names of the pis removed (remove_custom_words) and lowercased\n",
    "#remove_first_x_tokens: Remove starting phrases (and any tokens proceeding them up to \"x\") like 'description', 'provided', 'by', 'applicant'\n",
    "#remove_stopwords: Removes standard and user-entered custom stopwords. Note that this is not the text cleaning version to account for emptylists\n",
    "###############\n",
    "\n",
    "#Fill nulls to account for 2 blanks\n",
    "df['lemma_docs_with_stop'].fillna(value=np.nan,inplace=True)\n",
    "\n",
    "#Remove tokens of PI names from abstract, as well as lowercasing them\n",
    "no_pis=df.apply(lambda x: remove_custom_words(x,'lemma_docs_with_stop'),axis=1)\n",
    "print(no_pis)\n",
    "\n",
    "#Remove starting phrases (and any tokens proceeding them up to \"x\") like 'description', 'provided', 'by', 'applicant'\n",
    "#no_pis=no_pis.apply(remove_first_x_tokens,args=[start_phrases_to_remove])\n",
    "\n",
    "#Remove stopwords--nltk and those added on in 'additional_stopwords' function\n",
    "#stopWords = create_stopwords()\n",
    "#tokened_docs_nostop = TextCleaning.remove_stopwords(no_pis, stopWords) #Old code--does not account for non list documents\n",
    "#tokened_docs_nostop = no_pis.apply(remove_stopwords,args=[stopWords])\n",
    "#df['tokened_docs_nostop'] = tokened_docs_nostop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#Create bigrams and trigrams\n",
    "###################\n",
    "#Note that this is not trained on null abstracts\n",
    "bigram = gensim.models.Phrases(df['tokened_docs_nostop'].dropna(), min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "#This function will return a bigram if\n",
    "bigram_docs=df['tokened_docs_nostop'].apply(lambda x: apply_n_grams(x,bigram))\n",
    "trigram = gensim.models.Phrases(bigram_docs.dropna(), threshold=100)  \n",
    "tri_docs =bigram_docs.apply(lambda x: apply_n_grams(x,trigram))\n",
    "df['tns_bi_tri_docs'] = tri_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_tokens']=df['tns_bi_tri_docs'].apply(clean_up_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed text\n",
    "df.to_pickle(\"./processed_dataset_stanford_lemma.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Create datasets for analysis\n",
    "############\n",
    "\n",
    "#All data\n",
    "docs = df['final_tokens'].dropna() #<--If you're having issues merging with a prior dataset, note that this will NOT be the same length as the df overall, because of nulls, so be careful appending\n",
    "id2word, corpus = LDAvariables.createLDAvars(docs)\n",
    "pickle.dump([corpus, id2word, docs], open('lda_data_stanford_lemma.sav','wb'))\n",
    "\n",
    "#The two IDs for abstracts that were a space\n",
    "print('Abstract IDs with no lemmas in them')\n",
    "print(set(range(len(docs)))-set(docs.index))\n",
    "\n",
    "#Just nsf\n",
    "nsf_docs=df.groupby('AGENCY').get_group('NSF')['final_tokens'].dropna()\n",
    "id2word, corpus = LDAvariables.createLDAvars(nsf_docs)\n",
    "pickle.dump([corpus, id2word, nsf_docs], open('nsf_stanford_lemma.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old code method:\n",
    "\n",
    "#Data Cleaning\n",
    "#df[wa]=df[wa].apply(str.lower)\n",
    "#df[wa]=df.apply(remove_institution,axis=1) #case sensitive\n",
    "\n",
    "#Tokenizing\n",
    "#tokened_abstracts = TextCleaning.tokenize(df['working_abstract'])\n",
    "#df['tokened_abstracts'] = tokened_abstracts\n",
    "\n",
    "#Removal of stopwords, starting phrases, and PIs\n",
    "#no_pis=df.apply(remove_custom_words,axis=1).apply(remove_first_x_tokens,args=[start_phrases_to_remove])\n",
    "#stopWords = create_stopwords()\n",
    "#tokened_docs_nostop = TextCleaning.remove_stopwords(no_pis, stopWords)\n",
    "#df['tokened_docs_nostop'] = tokened_docs_nostop\n",
    "\n",
    "#n-grams\n",
    "#bigram = gensim.models.Phrases(df['tokened_docs_nostop'], min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "#bigram_docs=df['tokened_docs_nostop'].apply(lambda x: bigram[x])\n",
    "#trigram = gensim.models.Phrases(bigram_docs, threshold=100)  \n",
    "#tri_docs =bigram_docs.apply(lambda x: trigram[x])\n",
    "#df['tns_bi_tri_docs'] = tri_docs\n",
    "\n",
    "#Lemmatizing\n",
    "#lemma_docs = TextCleaning.lemmatize(df['tns_bi_tri_docs'])\n",
    "#df['lemma_abstracts'] = lemma_docs\n",
    "\n",
    "# save processed text\n",
    "\n",
    "#df.to_pickle(\"./processed_dataset.pkl\")\n",
    "#df.to_csv('FRAbstractsProcessed.csv')\n",
    "\n",
    "# Save only what is needed for LDA - docs, corpus, and dictionary. When loading the entire dataframe, I have run \n",
    "# out of memory to run the model\n",
    "\n",
    "# from Sam's code:\n",
    "#    corpus = corpus, dictionary = id2word, texts = docs\n",
    "\n",
    "#docs = df['lemma_abstracts']\n",
    "#id2word, corpus = LDAvariables.createLDAvars(docs)\n",
    "#pickle.dump([corpus, id2word, docs], open('lda_data.sav','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
